{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ7fqfptjS2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhLH66y0jTGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "code",
        "outputId": "81a04059-ca91-4be2-dd23-73e2d8f79f8b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYnDD30UjTRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile\n",
        "\n",
        "def collectDataFrom(parentPath, newsgroupList, wordCount=None):\n",
        "    data = []\n",
        "    for groupId, newsgroup in enumerate(newsgroupList):\n",
        "      dirPath = parentPath + '/' + newsgroup + '/'\n",
        "      files = [(fileName, dirPath + fileName)\n",
        "                for fileName in listdir(dirPath)\n",
        "                if isfile(dirPath + fileName)]\n",
        "      files.sort()\n",
        "      label = groupId\n",
        "      print('Processing: {}-{}'.format(groupId, newsgroup))\n",
        "\n",
        "      for fileName, filePath in files:\n",
        "          with open(filePath) as f:\n",
        "            text = f.read().lower()\n",
        "            words = re.split('\\W+', text)\n",
        "            if wordCount is not None:\n",
        "              for word in words:\n",
        "                wordCount[word] += 1\n",
        "            content = ' '.join(words)\n",
        "            assert len(content.splitlines()) == 1\n",
        "            data.append(str(label) + '<fff>'\n",
        "                        + fileName + '<fff>' + content)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqFnyENzjTcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile\n",
        "from collections import defaultdict\n",
        "\n",
        "def getDataAndVocab():\n",
        "    wordCount = defaultdict(int)\n",
        "    path = '/content/drive/My Drive/ML_DS_training/20news-bydate/'\n",
        "    parts = [path + dirName + '/' for dirName in listdir(path)\n",
        "             if not isfile(path + dirName)]\n",
        "\n",
        "    trainPath, testPath = (parts[0], parts[1]) \\\n",
        "        if 'train' in parts[0] else (parts[1], parts[0])\n",
        "\n",
        "    newsgroupList = [newsgroup for newsgroup in listdir(trainPath)]\n",
        "    newsgroupList.sort()\n",
        "\n",
        "    trainData = collectDataFrom(parentPath=trainPath,\n",
        "                               newsgroupList=newsgroupList,\n",
        "                               wordCount=wordCount)\n",
        "\n",
        "    vocab = [word for word, freq in zip(wordCount.keys(), wordCount.values()) \\\n",
        "             if freq > 10]\n",
        "    vocab.sort()\n",
        "    with open('/content/drive/ML_DS_training/datasets/w2v/vocab-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "\n",
        "    testData = collectDataFrom(parentPath=testPath,\n",
        "                              newsgroupList=newsgroupList)\n",
        "\n",
        "    with open('/content/drive/ML_DS_training/datasets/w2v/20news-train-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(trainData))\n",
        "    with open('/content/drive/ML_DS_training/datasets/w2v/20news-test-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(testData))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0n_hYe3jTob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b289101e-14be-4875-ad5a-f07ce8ab66a5"
      },
      "source": [
        "getDataAndVocab()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing: 0-alt.atheism\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-017294048c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetDataAndVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-5392519cca7e>\u001b[0m in \u001b[0;36mgetDataAndVocab\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     trainData = collectDataFrom(parentPath=trainPath,\n\u001b[1;32m     17\u001b[0m                                \u001b[0mnewsgroupList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewsgroupList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                wordCount=wordCount)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordCount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordCount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ee6e0074baeb>\u001b[0m in \u001b[0;36mcollectDataFrom\u001b[0;34m(parentPath, newsgroupList, wordCount)\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilePath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\W+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwordCount\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 12188: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fSe1nOpjTxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "unknownID = 0\n",
        "paddingID = 1\n",
        "\n",
        "def encodeData(dataPath, vocabPath):\n",
        "    with open(vocabPath) as f:\n",
        "        vocab = dict([(word, wordID + 2)\n",
        "                      for wordID, word in enumerate(f.read().splitlines())])\n",
        "    with open(dataPath) as f:\n",
        "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
        "                     for line in f.read().splitlines()]\n",
        "    encodedData = []\n",
        "    for document in documents:\n",
        "        label, docID, text = document\n",
        "        words = text.split()[:MAX_DOC_LENGTH]\n",
        "        sentenceLength = len(words)\n",
        "        encodedText = []\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                encodedText.append(str(vocab[word]))\n",
        "            else:\n",
        "                encodedText.append(str(unknownID))\n",
        "\n",
        "        if len(words) < MAX_DOC_LENGTH:\n",
        "            numPadding = MAX_DOC_LENGTH - len(words)\n",
        "            for _ in range(numPadding):\n",
        "                encodedText.append(str(paddingID))\n",
        "\n",
        "        encodedData.append(str(label) + '<fff>' + str(docID) + '<fff>' +\n",
        "                           str(sentenceLength) + '<fff>' + ' '.join(encodedText))\n",
        "\n",
        "    dirName = '/'.join(dataPath.split('/')[:-1])\n",
        "    fileName = '-'.join(dataPath.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "    with open(dirName + '/' + fileName, 'w') as f:\n",
        "        f.write('\\n'.join(encodedData))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adtezwajqbbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encodeData('My Drive/datasets/w2v/20news-train-raw.txt', 'My Drive/datasets/w2v/vocab-raw.txt')\n",
        "encodeData('My Drive/datasets/w2v/20news-test-raw.txt', 'My Drive/datasets/w2v/vocab-raw.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPSwgjDcs1Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzg1t7Cys7nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APBEiZ4ktFqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "    def __init__(self, vocabSize, embeddingSize, lstmSize, batchSize):\n",
        "        self._vocabSize = vocabSize\n",
        "        self._embeddingSize = embeddingSize\n",
        "        self._lstmSize = lstmSize\n",
        "        self._batchSize = batchSize\n",
        "\n",
        "        self._data = tf.placeholder(tf.int32, shape=[batchSize, MAX_DOC_LENGTH])\n",
        "        self._labels = tf.placeholder(tf.int32, shape=[batchSize, ])\n",
        "        self._sentence_lengths = tf.placeholder(tf.int32, shape=[batchSize, ])\n",
        "        self._finalTokens = tf.placeholder(tf.int32, shape=[batchSize, ])\n",
        "\n",
        "    def embeddingLayer(self, indices):\n",
        "        pretrainedVectors = [np.random.normal(loc=0, scale=1, size=self._embeddingSize)]\n",
        "        np.random.seed(2020)\n",
        "        for _ in range(self._vocabSize + 1):\n",
        "            pretrainedVectors.append(np.random.normal(loc=8, scale=1, size=self._embeddingSize))\n",
        "\n",
        "        pretrainedVectors = np.array(pretrainedVectors)\n",
        "        self._embeddingMatrix = tf.get_variable(\n",
        "            name='embedding',\n",
        "            shape=(self._vocabSize + 2, self._embeddingSize),\n",
        "            initializer=tf.constant_initializer(pretrainedVectors)\n",
        "        )\n",
        "\n",
        "        return tf.nn.embedding_lookup(self._embeddingMatrix, indices)\n",
        "\n",
        "    def LSTMLayer(self, embeddings):\n",
        "        lstmCell = tf.nn.rnn_cell.BasicLSTMCell(self._lstmSize)\n",
        "        zeroState = tf.zeros(shape=(self._batchSize, self._lstmSize))\n",
        "        initialState = tf.nn.rnn_cell.LSTMStateTuple(zeroState, zeroState)\n",
        "\n",
        "        lstmInputs = tf.unstack(tf.transpose(embeddings, perm=[1, 0, 2]))\n",
        "        lstmOutputs, lastState = tf.nn.static_rnn(\n",
        "            cell=lstmCell,\n",
        "            inputs=lstmInputs,\n",
        "            initial_state=initialState,\n",
        "            sequence_length=self._sentence_lengths\n",
        "        )\n",
        "        lstmOutputs = tf.unstack(\n",
        "            tf.compat.v1.transpose(lstmOutputs, perm=[1, 0, 2])\n",
        "        )\n",
        "        lstmOutputs = tf.concat(\n",
        "            lstmOutputs, axis=0\n",
        "        )  # [num docs * MAX_SENT_LENGTH, lstm_size]\n",
        "\n",
        "        # self._mask : [num docs * MAX_SENT_LENGTH, ]\n",
        "        mask = tf.sequence_mask(\n",
        "            lengths=self._sentence_lengths,\n",
        "            maxlen=MAX_DOC_LENGTH,\n",
        "            dtype=tf.compat.v1.float32\n",
        "        )  # [num docs, MAX_SENTENCE_LENGTH]\n",
        "        mask = tf.concat(tf.compat.v1.unstack(mask, axis=0), axis=0)\n",
        "        mask = tf.expand_dims(mask, -1)\n",
        "        lstmOutputs = mask * lstmOutputs\n",
        "        lstmOutputsSplit = tf.split(lstmOutputs, num_or_size_splits=self._batchSize)\n",
        "        lstmOutputsSum = tf.reduce_sum(lstmOutputsSplit, axis=1)  # [ num_docs, lstm_size]\n",
        "        lstmOutputsAverage = lstmOutputsSum / tf.expand_dims(\n",
        "            tf.cast(self._sentence_lengths, tf.float32),\n",
        "            # expand_dims only works with tensor of float type\n",
        "            -1)  # [num_docs, lstm_size]\n",
        "        return lstmOutputsAverage\n",
        "\n",
        "    def buildGraph(self):\n",
        "        embeddings = self.embeddingLayer(self._data)\n",
        "        lstmOutputs = self.LSTMLayer(embeddings)\n",
        "\n",
        "        weights = tf.cget_variable(\n",
        "            name='final_layer_weights',\n",
        "            shape=(self._lstmSize, NUM_CLASSES),\n",
        "            initializer=tf.random_normal_initializer(seed=2020)\n",
        "        )\n",
        "        biases = tf.get_variable(\n",
        "            name='final_layer_biases',\n",
        "            shape=NUM_CLASSES,\n",
        "            initializer=tf.random_normal_initializer(seed=2020)\n",
        "        )\n",
        "\n",
        "        logits = tf.matmul(lstmOutputs, weights) + biases\n",
        "        labels_one_hot = tf.one_hot(\n",
        "            indices=self._labels,\n",
        "            depth=NUM_CLASSES,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "            labels=labels_one_hot,\n",
        "            logits=logits\n",
        "        )\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        predictedLabels = tf.argmax(probs, axis=1)\n",
        "        predictedLabels = tf.squeeze(predictedLabels)\n",
        "\n",
        "        return predictedLabels, loss\n",
        "\n",
        "    def trainer(self, loss, learningRate):\n",
        "        trainOp = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
        "        return trainOp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Z3YSA7uue3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataReader:\n",
        "    def __init__(self, dataPath, batchSize, vocabSize):\n",
        "        self._batch_size = batchSize\n",
        "        with open(dataPath) as f:\n",
        "            lines = f.read().splitlines()\n",
        "\n",
        "        self._data = []\n",
        "        self._labels = []\n",
        "        self._sentence_lengths = []\n",
        "\n",
        "        for data_id, data_d in enumerate(lines):\n",
        "            if len(data_d) > 1:\n",
        "                feature = data_d.split('<fff>')\n",
        "                label, doc_id, sentenceLength = int(feature[0]), int(feature[1]), int(feature[2])\n",
        "                vecs = feature[3].split()\n",
        "                vector = [int(vec) for vec in vecs]\n",
        "                self._data.append(vector)\n",
        "                self._labels.append(label)\n",
        "                self._sentence_lengths.append(sentenceLength)\n",
        "\n",
        "        self._data = np.array(self._data)\n",
        "        self._labels = np.array(self._labels)\n",
        "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "\n",
        "        self._num_epoch = 0\n",
        "        self._batch_id = 0\n",
        "\n",
        "    def nextBatch(self):\n",
        "        start = self._batch_size * self._batch_id\n",
        "        end = start + self._batch_size\n",
        "        self._batch_id += 1\n",
        "        if end > len(self._data):\n",
        "            end = len(self._data)\n",
        "            self._num_epoch += 1\n",
        "            self._batch_id = 0\n",
        "\n",
        "            arr = np.array(range(len(self._data)))\n",
        "            np.random.seed(2020)\n",
        "            np.random.shuffle(arr)\n",
        "\n",
        "            self._data, self._labels, self._sentence_lengths = self._data[arr], self._labels[arr], \\\n",
        "                                                               self._sentence_lengths[arr]\n",
        "\n",
        "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_261RjJP11qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainAndEvaluateRNN():\n",
        "    with open('My Drive/ML_DS_training/datasets/w2v/vocab-raw.txt') as f:\n",
        "        vocabSize = len(f.read().splitlines())\n",
        "\n",
        "    tf.set_random_seed(2020)\n",
        "    rnn = RNN(\n",
        "        vocabSize=vocabSize,\n",
        "        embeddingSize=300,\n",
        "        lstmSize=50,\n",
        "        batchSize=50\n",
        "    )\n",
        "    predictedLabels, loss = rnn.buildGraph()\n",
        "    trainOp = rnn.trainer(loss=loss, learningRate=0.01)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        trainDataReader = DataReader(\n",
        "            dataPath='My Drive/ML_DS_training/datasets/w2v/20news-train-encoded.txt',\n",
        "            batchSize=50,\n",
        "            vocabSize=vocabSize\n",
        "        )\n",
        "\n",
        "        testDataReader = DataReader(\n",
        "            dataPath='My Drive/ML_DS_training/datasets/w2v/20news-test-encoded.txt',\n",
        "            batchSize=50,\n",
        "            vocabSize=vocabSize\n",
        "        )\n",
        "\n",
        "        step = 0\n",
        "        MAX_STEP = 2000\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        while step < MAX_STEP:\n",
        "            nextTrainBatch = trainDataReader.nextBatch()\n",
        "            trainData, trainLabels, trainSentenceLengths = nextTrainBatch\n",
        "            plabelsEval, lossEval, _ = sess.run(\n",
        "                [predictedLabels, loss, trainOp],\n",
        "                feed_dict={\n",
        "                    rnn._data: trainData,\n",
        "                    rnn._labels: trainLabels,\n",
        "                    rnn._sentence_lengths: trainSentenceLengths,\n",
        "                }\n",
        "            )\n",
        "            step += 1\n",
        "            if step % 20 == 0:\n",
        "                print(\"step: \" + str(step) + \" loss: \" + str(lossEval))\n",
        "            if trainDataReader._batch_id == 0:\n",
        "                numTruePreds = 0\n",
        "                while True:\n",
        "                    nextTestBatch = testDataReader.nextBatch()\n",
        "                    testData, testLabels, testSentenceLengths = nextTestBatch\n",
        "\n",
        "                    testPlabelsEval, lossEval, _ = sess.run(\n",
        "                        [predictedLabels, loss, trainOp],\n",
        "                        feed_dict={\n",
        "                            rnn._data: testData,\n",
        "                            rnn._labels: testLabels,\n",
        "                            rnn._sentence_lengths: testSentenceLengths,\n",
        "                        }\n",
        "                    )\n",
        "                    matches = np.equal(testPlabelsEval, testLabels)\n",
        "                    numTruePreds += np.sum(matches.astype(float))\n",
        "\n",
        "                    if testDataReader._batch_id == 0: break\n",
        "                print('Epoch: ', trainDataReader._num_epoch)\n",
        "                print('Accuracy on test data: ', numTruePreds * 100. / len(testDataReader._data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_74LrHFO2ALX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf..variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        trainAndEvaluateRNN()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}