{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ7fqfptjS2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhLH66y0jTGL",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "c68d3e0c-d3f1-47be-ef3a-fb06cafc568a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYnDD30UjTRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def collectDataFrom(parentPath, newsgroupList, wordCount=None):\n",
        "        data = []\n",
        "        for groupId, newsgroup in enumerate(newsgroupList):\n",
        "            dirPath = parentPath + '/' + newsgroup + '/'\n",
        "            files = [(fileName, dirPath + fileName)\n",
        "                     for fileName in listdir(dirPath)\n",
        "                     if isfile(dirPath + fileName)]\n",
        "            files.sort()\n",
        "            label = groupId\n",
        "            print('Processing: {}-{}'.format(groupId, newsgroup))\n",
        "\n",
        "            for fileName, filePath in files:\n",
        "                with open(filePath, 'r', encoding = 'utf-8', errors='ignore') as f:\n",
        "                    text = f.read().lower()\n",
        "                    words = re.split('\\W+', text)\n",
        "                    if wordCount is not None:\n",
        "                        for word in words:\n",
        "                            wordCount[word] += 1\n",
        "                    content = ' '.join(words)\n",
        "                    assert len(content.splitlines()) == 1\n",
        "                    data.append(str(label) + '<fff>'\n",
        "                                + fileName + '<fff>' + content)\n",
        "\n",
        "        return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqFnyENzjTcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile\n",
        "from collections import defaultdict\n",
        "\n",
        "def getDataAndVocab():\n",
        "    wordCount = defaultdict(int)\n",
        "    path = '/content/drive/My Drive/ML_DS_training/20news-bydate/'\n",
        "    parts = [path + dirName + '/' for dirName in listdir(path)\n",
        "             if not isfile(path + dirName)]\n",
        "\n",
        "    trainPath, testPath = (parts[0], parts[1]) \\\n",
        "        if 'train' in parts[0] else (parts[1], parts[0])\n",
        "\n",
        "    newsgroupList = [newsgroup for newsgroup in listdir(trainPath)]\n",
        "    newsgroupList.sort()\n",
        "\n",
        "    trainData = collectDataFrom(parentPath=trainPath,\n",
        "                               newsgroupList=newsgroupList,\n",
        "                                wordCount=wordCount)\n",
        "\n",
        "    vocab = [word for word, freq in zip(wordCount.keys(), wordCount.values()) \\\n",
        "             if freq > 10]\n",
        "    vocab.sort()\n",
        "    with open('/content/drive/My Drive/ML_DS_training/datasets/w2v/vocab-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "\n",
        "    testData = collectDataFrom(parentPath=testPath,\n",
        "                              newsgroupList=newsgroupList)\n",
        "\n",
        "    with open('/content/drive/My Drive/ML_DS_training/datasets/w2v/20news-train-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(trainData))\n",
        "    with open('/content/drive/My Drive/ML_DS_training/datasets/w2v/20news-test-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(testData))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0n_hYe3jTob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getDataAndVocab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fSe1nOpjTxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "unknownID = 0\n",
        "paddingID = 1\n",
        "\n",
        "def encodeData(dataPath, vocabPath):\n",
        "    with open(vocabPath) as f:\n",
        "        vocab = dict([(word, wordID + 2)\n",
        "                      for wordID, word in enumerate(f.read().splitlines())])\n",
        "    with open(dataPath) as f:\n",
        "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
        "                     for line in f.read().splitlines()]\n",
        "    encodedData = []\n",
        "    for document in documents:\n",
        "        label, docID, text = document\n",
        "        words = text.split()[:MAX_DOC_LENGTH]\n",
        "        sentenceLength = len(words)\n",
        "        encodedText = []\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                encodedText.append(str(vocab[word]))\n",
        "            else:\n",
        "                encodedText.append(str(unknownID))\n",
        "\n",
        "        if len(words) < MAX_DOC_LENGTH:\n",
        "            numPadding = MAX_DOC_LENGTH - len(words)\n",
        "            for _ in range(numPadding):\n",
        "                encodedText.append(str(paddingID))\n",
        "\n",
        "        encodedData.append(str(label) + '<fff>' + str(docID) + '<fff>' +\n",
        "                           str(sentenceLength) + '<fff>' + ' '.join(encodedText))\n",
        "\n",
        "    dirName = '/'.join(dataPath.split('/')[:-1])\n",
        "    fileName = '-'.join(dataPath.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "    with open(dirName + '/' + fileName, 'w') as f:\n",
        "        f.write('\\n'.join(encodedData))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adtezwajqbbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encodeData('/content/drive/My Drive/ML_DS_training/datasets/w2v/20news-train-raw.txt', '/content/drive/My Drive/ML_DS_training/datasets/w2v/vocab-raw.txt')\n",
        "encodeData('/content/drive/My Drive/ML_DS_training/datasets/w2v/20news-test-raw.txt', '/content/drive/My Drive/ML_DS_training/datasets/w2v/vocab-raw.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPSwgjDcs1Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzg1t7Cys7nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APBEiZ4ktFqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "    def __init__(self, vocabSize, embeddingSize, lstmSize, batchSize):\n",
        "        self._vocabSize = vocabSize\n",
        "        self._embeddingSize = embeddingSize\n",
        "        self._lstmSize = lstmSize\n",
        "        self._batchSize = batchSize\n",
        "\n",
        "        self._data = tf.placeholder(tf.int32, shape=[batchSize, MAX_DOC_LENGTH])\n",
        "        self._labels = tf.placeholder(tf.int32, shape=[batchSize, ])\n",
        "        self._sentence_lengths = tf.placeholder(tf.int32, shape=[batchSize, ])\n",
        "        self._finalTokens = tf.placeholder(tf.int32, shape=[batchSize, ])\n",
        "\n",
        "    def embeddingLayer(self, indices):\n",
        "        pretrainedVectors= []\n",
        "        pretrainedVectors.append(np.zeros(self._embeddingSize))\n",
        "        np.random.seed(2020)\n",
        "        for _ in range(self._vocabSize + 1):\n",
        "            pretrainedVectors.append(np.random.normal(loc=0., scale=1., size=self._embeddingSize))\n",
        "\n",
        "        pretrainedVectors = np.array(pretrainedVectors)\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            self._embeddingMatrix = tf.get_variable(\n",
        "              name= 'embedding',\n",
        "              shape= (self._vocabSize + 2, self._embeddingSize),\n",
        "              initializer= tf.constant_initializer(pretrainedVectors)\n",
        "            )\n",
        "\n",
        "        return tf.nn.embedding_lookup(self._embeddingMatrix, indices)\n",
        "\n",
        "    def LSTMLayer(self, embeddings):\n",
        "        lstmCell = tf.nn.rnn_cell.BasicLSTMCell(self._lstmSize)\n",
        "        zeroState = tf.zeros(shape=(self._batchSize, self._lstmSize))\n",
        "        initialState = tf.nn.rnn_cell.LSTMStateTuple(zeroState, zeroState)\n",
        "\n",
        "        lstmInputs = tf.unstack(tf.transpose(embeddings, perm=[1, 0, 2]))\n",
        "        lstmOutputs, lastState = tf.nn.static_rnn(\n",
        "            cell=lstmCell,\n",
        "            inputs=lstmInputs,\n",
        "            initial_state=initialState,\n",
        "            sequence_length=self._sentence_lengths\n",
        "        )\n",
        "        lstmOutputs = tf.unstack(\n",
        "            tf.transpose(lstmOutputs, perm=[1, 0, 2])\n",
        "        )\n",
        "        lstmOutputs = tf.concat(\n",
        "            lstmOutputs, axis=0\n",
        "        )  # [num docs * MAX_SENT_LENGTH, lstm_size]\n",
        "\n",
        "        # self._mask : [num docs * MAX_SENT_LENGTH, ]\n",
        "        mask = tf.sequence_mask(\n",
        "            lengths=self._sentence_lengths,\n",
        "            maxlen=MAX_DOC_LENGTH,\n",
        "            dtype=tf.float32\n",
        "        )  # [num docs, MAX_SENTENCE_LENGTH]\n",
        "        mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "        mask = tf.expand_dims(mask, -1)\n",
        "        lstmOutputs = mask * lstmOutputs\n",
        "        lstmOutputsSplit = tf.split(lstmOutputs, num_or_size_splits=self._batchSize)\n",
        "        lstmOutputsSum = tf.reduce_sum(lstmOutputsSplit, axis=1)  # [ num_docs, lstm_size]\n",
        "        lstmOutputsAverage = lstmOutputsSum / tf.expand_dims(\n",
        "            tf.cast(self._sentence_lengths, tf.float32),\n",
        "            # expand_dims only works with tensor of float type\n",
        "            -1)  # [num_docs, lstm_size]\n",
        "        return lstmOutputsAverage\n",
        "\n",
        "    def buildGraph(self):\n",
        "        embeddings = self.embeddingLayer(self._data)\n",
        "        lstmOutputs = self.LSTMLayer(embeddings)\n",
        "\n",
        "        weights = tf.get_variable(\n",
        "            name='final_layer_weights',\n",
        "            shape=(self._lstmSize, NUM_CLASSES),\n",
        "            initializer=tf.random_normal_initializer(seed=2020)\n",
        "        )\n",
        "        biases = tf.get_variable(\n",
        "            name='final_layer_biases',\n",
        "            shape=NUM_CLASSES,\n",
        "            initializer=tf.random_normal_initializer(seed=2020)\n",
        "        )\n",
        "\n",
        "        logits = tf.matmul(lstmOutputs, weights) + biases\n",
        "        labels_one_hot = tf.one_hot(\n",
        "            indices=self._labels,\n",
        "            depth=NUM_CLASSES,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=labels_one_hot,\n",
        "            logits=logits\n",
        "        )\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        predictedLabels = tf.argmax(probs, axis=1)\n",
        "        predictedLabels = tf.squeeze(predictedLabels)\n",
        "\n",
        "        return predictedLabels, loss\n",
        "\n",
        "    def trainer(self, loss, learningRate):\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "          trainOp = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
        "        return trainOp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Z3YSA7uue3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataReader:\n",
        "    def __init__(self, dataPath, batchSize, vocabSize):\n",
        "        self._batch_size = batchSize\n",
        "        with open(dataPath) as f:\n",
        "            lines = f.read().splitlines()\n",
        "\n",
        "        self._data = []\n",
        "        self._labels = []\n",
        "        self._sentence_lengths = []\n",
        "\n",
        "        for data_id, data_d in enumerate(lines):\n",
        "            if len(data_d) > 1:\n",
        "                (label, doc_id, sentenceLength, vec) = data_d.split(\"<fff>\")\n",
        "                label = int(label)\n",
        "                doc_id = int(doc_id)\n",
        "                sentenceLength = int(sentenceLength)\n",
        "                vector = [int(ele) for ele in vec.split()]\n",
        "\n",
        "                self._data.append(vector)\n",
        "                self._labels.append(label)\n",
        "                self._sentence_lengths.append(sentenceLength)\n",
        "\n",
        "        self._data = np.array(self._data)\n",
        "        self._labels = np.array(self._labels)\n",
        "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "\n",
        "        self._num_epoch = 0\n",
        "        self._batch_id = 0\n",
        "\n",
        "    def nextBatch(self):\n",
        "        start = self._batch_size * self._batch_id\n",
        "        end = start + self._batch_size\n",
        "        self._batch_id += 1\n",
        "        if end > len(self._data):\n",
        "            end = len(self._data)\n",
        "            self._num_epoch += 1\n",
        "            self._batch_id = 0\n",
        "\n",
        "            arr = np.array(range(len(self._data)))\n",
        "            np.random.seed(2020)\n",
        "            np.random.shuffle(arr)\n",
        "\n",
        "            self._data, self._labels, self._sentence_lengths = self._data[arr], self._labels[arr], \\\n",
        "                                                               self._sentence_lengths[arr]\n",
        "\n",
        "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_261RjJP11qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainAndEvaluateRNN():\n",
        "    with open('/content/drive/My Drive/ML_DS_training/datasets/w2v/vocab-raw.txt') as f:\n",
        "        vocabSize = len(f.read().splitlines())\n",
        "\n",
        "    tf.set_random_seed(2020)\n",
        "    rnn = RNN(\n",
        "        vocabSize=vocabSize,\n",
        "        embeddingSize=300,\n",
        "        lstmSize=50,\n",
        "        batchSize=50\n",
        "    )\n",
        "    predictedLabels, loss = rnn.buildGraph()\n",
        "    trainOp = rnn.trainer(loss=loss, learningRate=0.01)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        trainDataReader = DataReader(\n",
        "            dataPath='/content/drive/My Drive/ML_DS_training/datasets/w2v/20news-train-encoded.txt',\n",
        "            batchSize=50,\n",
        "            vocabSize=vocabSize\n",
        "        )\n",
        "\n",
        "        testDataReader = DataReader(\n",
        "            dataPath='/content/drive/My Drive/ML_DS_training/datasets/w2v/20news-test-encoded.txt',\n",
        "            batchSize=50,\n",
        "            vocabSize=vocabSize\n",
        "        )\n",
        "\n",
        "        step = 0\n",
        "        MAX_STEP = 1500\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        while step < MAX_STEP:\n",
        "            nextTrainBatch = trainDataReader.nextBatch()\n",
        "            trainData, trainLabels, trainSentenceLengths = nextTrainBatch\n",
        "            plabelsEval, lossEval, _ = sess.run(\n",
        "                [predictedLabels, loss, trainOp],\n",
        "                feed_dict={\n",
        "                    rnn._data: trainData,\n",
        "                    rnn._labels: trainLabels,\n",
        "                    rnn._sentence_lengths: trainSentenceLengths,\n",
        "                }\n",
        "            )\n",
        "            step += 1\n",
        "            if step % 20 == 0:\n",
        "                print(\"step: \" + str(step) + \" loss: \" + str(lossEval))\n",
        "            if trainDataReader._batch_id == 0:\n",
        "                numTruePreds = 0\n",
        "                while True:\n",
        "                    nextTestBatch = testDataReader.nextBatch()\n",
        "                    testData, testLabels, testSentenceLengths = nextTestBatch\n",
        "\n",
        "                    testPlabelsEval, lossEval, _ = sess.run(\n",
        "                        [predictedLabels, loss, trainOp],\n",
        "                        feed_dict={\n",
        "                            rnn._data: testData,\n",
        "                            rnn._labels: testLabels,\n",
        "                            rnn._sentence_lengths: testSentenceLengths,\n",
        "                        }\n",
        "                    )\n",
        "                    matches = np.equal(testPlabelsEval, testLabels)\n",
        "                    numTruePreds += np.sum(matches.astype(float))\n",
        "\n",
        "                    if testDataReader._batch_id == 0: break\n",
        "                print('Epoch: ', trainDataReader._num_epoch)\n",
        "                print('Accuracy on test data: ', numTruePreds * 100. / len(testDataReader._data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_74LrHFO2ALX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        trainAndEvaluateRNN()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}